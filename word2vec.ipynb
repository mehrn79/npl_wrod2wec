{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrn79/npl_wrod2wec/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCfjbt1e7-4X"
      },
      "source": [
        "# **word2vec2 implematation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYdzbOsG8RQI"
      },
      "source": [
        "## **preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSXh9vjA8kWs"
      },
      "source": [
        "### **libraries for preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OJ_ZJnknMjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73e83e2-b698-4595-eda8-86799648658b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 56.9 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=7a67e3b7cbbb6e79ec9ad4c4208091564dcbe52886a9ad80163b9fab7e8d9bb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154066 sha256=4f4642ecdb86df45acc68c338b9cddfba9f80cd643fc3495cb6fe34e77be7092\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBFYO3kdnN9X"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from keras.preprocessing import text\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv56oW6W87Gh"
      },
      "source": [
        "### **getting the corpes & stop words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KKiOLfJmUxW"
      },
      "outputs": [],
      "source": [
        "with open('/content/shams.txt') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "with open('/content/stopwords.txt') as file:\n",
        "  stopLines = file.readlines()\n",
        "stopWord = [item.replace('\\n',\"\") for item in stopLines]\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioJaG4I29efJ"
      },
      "source": [
        "### **cleaning the corpes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfuBbbxUTMa_"
      },
      "outputs": [],
      "source": [
        "words= []\n",
        "stemmer = Stemmer()\n",
        "for sent in lines :\n",
        "  words.append(word_tokenize(sent))\n",
        "\n",
        "text_word_list=[]\n",
        "corpes=[]\n",
        "new_words=[]\n",
        "\n",
        "for wordSent in words :\n",
        "  cleanSent=[]\n",
        "  for word in wordSent :\n",
        "    if word not in stopWord :\n",
        "      cleanSent.append(stemmer.stem(word))\n",
        "  if(len(cleanSent)>1) :\n",
        "    text_word_list+=cleanSent\n",
        "    new_words.append(cleanSent)\n",
        "\n",
        "value_of_repetition = dict(Counter(text_word_list))\n",
        "repetition=[]\n",
        "for word in value_of_repetition :\n",
        "  if value_of_repetition.get(word)==1:\n",
        "    repetition.append(word)\n",
        "clear_word_list=[x for x in text_word_list if x not in repetition]\n",
        "\n",
        "text_word_list=[]\n",
        "\n",
        "for wordSent in new_words :\n",
        "  cleanSent=[]\n",
        "  for word in wordSent :\n",
        "    if word not in repetition :\n",
        "      cleanSent.append(word)\n",
        "  if(len(cleanSent)>1) :\n",
        "    text_word_list+=cleanSent\n",
        "    corpes.append(' '.join(cleanSent))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **tokenizing the corpes**"
      ],
      "metadata": {
        "id": "-DoYRglAw-SL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5y5E5G1UY3j"
      },
      "outputs": [],
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpes)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7-ahDuv9-93"
      },
      "source": [
        "### **creating list of target words and context words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC77WNedoFY3"
      },
      "outputs": [],
      "source": [
        "window = 2\n",
        "word_lists = []\n",
        "\n",
        "for text in corpes:\n",
        "    text=word_tokenize(text)\n",
        "\n",
        "    for i, word in enumerate(text):\n",
        "        for w in range(window):\n",
        "            if i + 1 + w < len(text): \n",
        "                word_lists.append([word] + [text[(i + 1 + w)]])   \n",
        "            if i - w - 1 >= 0:\n",
        "                word_lists.append([word] + [text[(i - w - 1)]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tuvsBY0-Wom"
      },
      "source": [
        "### **finding unique words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wtkv4ojtjdC"
      },
      "outputs": [],
      "source": [
        "words_set = list(set(text_word_list))\n",
        "words_set.sort()\n",
        "\n",
        "unique_word_dict = {}\n",
        "for i, word in enumerate(words_set):\n",
        "  unique_word_dict.update({\n",
        "    word: i\n",
        "  })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ9o9TtB-2Ud"
      },
      "source": [
        "## **make X & Y of train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFm6FQJJyfXF",
        "outputId": "389d06af-f28b-4a2c-e552-070a1b5d5bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "110306it [00:04, 22552.61it/s]\n"
          ]
        }
      ],
      "source": [
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "n_words = len(unique_word_dict)\n",
        " \n",
        "words = list(unique_word_dict.keys())\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i, word_list in tqdm(enumerate(word_lists)):\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\n",
        " \n",
        "    X_row = np.zeros(n_words)\n",
        "    Y_row = np.zeros(n_words)\n",
        "\n",
        "    X_row[main_word_index] = 1\n",
        "    Y_row[context_word_index] = 1\n",
        "\n",
        "    X.append(X_row)\n",
        "    Y.append(Y_row)\n",
        "\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhlq-DpH_QeO"
      },
      "source": [
        "## **creating training model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29-u_XCcbR0a",
        "outputId": "582c395d-bda9-4c4b-c935-950f7c53ae6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3447)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 15)                51720     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3447)              55152     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 106,872\n",
            "Trainable params: 106,872\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.9003e-04 - categorical_accuracy: 0.0221\n",
            "Epoch 2/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.9003e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 3/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.9002e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 4/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.9002e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 5/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.9001e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 6/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.9000e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 7/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.8998e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 8/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.8974e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 9/20\n",
            "3448/3448 [==============================] - 20s 6ms/step - loss: 2.8967e-04 - categorical_accuracy: 0.0248\n",
            "Epoch 10/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.8963e-04 - categorical_accuracy: 0.0248\n",
            "Epoch 11/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.8961e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 12/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.8959e-04 - categorical_accuracy: 0.0249\n",
            "Epoch 13/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.8955e-04 - categorical_accuracy: 0.0252\n",
            "Epoch 14/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.8951e-04 - categorical_accuracy: 0.0254\n",
            "Epoch 15/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.8947e-04 - categorical_accuracy: 0.0257\n",
            "Epoch 16/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.8944e-04 - categorical_accuracy: 0.0264\n",
            "Epoch 17/20\n",
            "3448/3448 [==============================] - 19s 6ms/step - loss: 2.8942e-04 - categorical_accuracy: 0.0267\n",
            "Epoch 18/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.8940e-04 - categorical_accuracy: 0.0276\n",
            "Epoch 19/20\n",
            "3448/3448 [==============================] - 18s 5ms/step - loss: 2.8938e-04 - categorical_accuracy: 0.0278\n",
            "Epoch 20/20\n",
            "3448/3448 [==============================] - 19s 5ms/step - loss: 2.8936e-04 - categorical_accuracy: 0.0283\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcfe3beb3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from keras.models import Input, Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "embed_size = 15\n",
        "\n",
        "inp = Input(shape=(X.shape[1],))\n",
        "x = Dense(units=embed_size, activation='linear')(inp)\n",
        "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss = 'mean_squared_error', optimizer = 'Adam', metrics=['categorical_accuracy'])\n",
        "model.summary()\n",
        "model.fit(\n",
        "    x=X, \n",
        "    y=Y, \n",
        "    epochs=20,\n",
        "    verbose=1\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a dictionary to store the embeddings**"
      ],
      "metadata": {
        "id": "eQzHirhgPyZ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MzZNxEROCAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb04501d-e197-466e-9a9e-f0e9f31ea995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.09252767  0.06050421  0.00355326  0.13323204  0.0607237   0.02255718\n",
            "  0.09553705  0.03446832 -0.00841697  0.06273373 -0.07020693  0.0751634\n",
            "  0.0452446  -0.08023857  0.1716735 ]\n"
          ]
        }
      ],
      "source": [
        "weights = model.get_weights()[0]\n",
        "\n",
        "embedding_dict = {}\n",
        "for word in words: \n",
        "    embedding_dict.update({\n",
        "        word: weights[unique_word_dict.get(word)]\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **return similar words**"
      ],
      "metadata": {
        "id": "d3QaPcViUgDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "4YswfHfLSH55"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def the_most_similar_words(word) :\n",
        "  distance_matrix = euclidean_distances(weights)\n",
        "  similar_words = {search_term: [list(unique_word_dict.keys())[idx] for idx in distance_matrix[unique_word_dict[search_term]-1].argsort()[1:15]+1] \n",
        "                   for search_term in [word]}\n",
        "\n",
        "  return similar_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "the_most_similar_words('شمس')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8bhghT8CkVz",
        "outputId": "3d7082f7-0ca1-4d42-8772-5b5c4c264d32"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'شمس': ['نیاف',\n",
              "  'قمارخانه',\n",
              "  'همتا',\n",
              "  'زشت',\n",
              "  'گریس',\n",
              "  'پرگهر',\n",
              "  'خریدار',\n",
              "  'رمد',\n",
              "  'جرم',\n",
              "  'اشتاب',\n",
              "  'خموش',\n",
              "  'مسل',\n",
              "  'همنفس',\n",
              "  'رطل']}"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "word2vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+emjGMh9yw1zuxIsXP36Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}