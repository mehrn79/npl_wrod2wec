{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrn79/npl_wrod2wec/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCfjbt1e7-4X"
      },
      "source": [
        "# **word2vec2 implematation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYdzbOsG8RQI"
      },
      "source": [
        "## **preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSXh9vjA8kWs"
      },
      "source": [
        "### **libraries for preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OJ_ZJnknMjH"
      },
      "outputs": [],
      "source": [
        "pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kBFYO3kdnN9X"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from keras.preprocessing import text\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv56oW6W87Gh"
      },
      "source": [
        "### **getting the corpes & stop words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7KKiOLfJmUxW"
      },
      "outputs": [],
      "source": [
        "with open('/content/shams.txt') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "with open('/content/stopwords.txt') as file:\n",
        "  stopLines = file.readlines()\n",
        "stopWord = [item.replace('\\n',\"\") for item in stopLines]\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioJaG4I29efJ"
      },
      "source": [
        "### **cleaning the corpes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZfuBbbxUTMa_"
      },
      "outputs": [],
      "source": [
        "words= []\n",
        "stemmer = Stemmer()\n",
        "for sent in lines :\n",
        "  words.append(word_tokenize(sent))\n",
        "\n",
        "text_word_list=[]\n",
        "corpes=[]\n",
        "new_words=[]\n",
        "\n",
        "for wordSent in words :\n",
        "  cleanSent=[]\n",
        "  for word in wordSent :\n",
        "    if word not in stopWord :\n",
        "      cleanSent.append(stemmer.stem(word))\n",
        "  if(len(cleanSent)>1) :\n",
        "    text_word_list+=cleanSent\n",
        "    new_words.append(cleanSent)\n",
        "\n",
        "value_of_repetition = dict(Counter(text_word_list))\n",
        "repetition=[]\n",
        "for word in value_of_repetition :\n",
        "  if value_of_repetition.get(word)==1:\n",
        "    repetition.append(word)\n",
        "clear_word_list=[x for x in text_word_list if x not in repetition]\n",
        "\n",
        "text_word_list=[]\n",
        "\n",
        "for wordSent in new_words :\n",
        "  cleanSent=[]\n",
        "  for word in wordSent :\n",
        "    if word not in repetition :\n",
        "      cleanSent.append(word)\n",
        "  if(len(cleanSent)>1) :\n",
        "    text_word_list+=cleanSent\n",
        "    corpes.append(' '.join(cleanSent))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **tokenizing the corpes**"
      ],
      "metadata": {
        "id": "-DoYRglAw-SL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "p5y5E5G1UY3j"
      },
      "outputs": [],
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpes)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7-ahDuv9-93"
      },
      "source": [
        "### **creating list of target words and context words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC77WNedoFY3"
      },
      "outputs": [],
      "source": [
        "window = 2\n",
        "word_lists = []\n",
        "\n",
        "for text in corpes:\n",
        "    text=word_tokenize(text)\n",
        "\n",
        "    for i, word in enumerate(text):\n",
        "        for w in range(window):\n",
        "            if i + 1 + w < len(text): \n",
        "                word_lists.append([word] + [text[(i + 1 + w)]])   \n",
        "            if i - w - 1 >= 0:\n",
        "                word_lists.append([word] + [text[(i - w - 1)]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tuvsBY0-Wom"
      },
      "source": [
        "### **finding unique words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7Wtkv4ojtjdC"
      },
      "outputs": [],
      "source": [
        "words_set = list(set(text_word_list))\n",
        "words_set.sort()\n",
        "\n",
        "unique_word_dict = {}\n",
        "for i, word in enumerate(words_set):\n",
        "  unique_word_dict.update({\n",
        "    word: i\n",
        "  })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ9o9TtB-2Ud"
      },
      "source": [
        "## **make X & Y of train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFm6FQJJyfXF",
        "outputId": "faca1c9b-a3f2-481d-e48d-56b570160128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "110306it [00:05, 22042.72it/s]\n"
          ]
        }
      ],
      "source": [
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "n_words = len(unique_word_dict)\n",
        " \n",
        "words = list(unique_word_dict.keys())\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i, word_list in tqdm(enumerate(word_lists)):\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\n",
        " \n",
        "    X_row = np.zeros(n_words)\n",
        "    Y_row = np.zeros(n_words)\n",
        "\n",
        "    X_row[main_word_index] = 1\n",
        "    Y_row[context_word_index] = 1\n",
        "\n",
        "    X.append(X_row)\n",
        "    Y.append(Y_row)\n",
        "\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhlq-DpH_QeO"
      },
      "source": [
        "## **creating training model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29-u_XCcbR0a",
        "outputId": "2d6985c5-a944-4e11-ccf9-d75f65568474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3447)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               344800    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3447)              348147    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 692,947\n",
            "Trainable params: 692,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "3448/3448 [==============================] - 34s 10ms/step - loss: 7.2915 - categorical_accuracy: 0.0258\n",
            "Epoch 2/20\n",
            "3448/3448 [==============================] - 34s 10ms/step - loss: 7.0847 - categorical_accuracy: 0.0331\n",
            "Epoch 3/20\n",
            "3448/3448 [==============================] - 34s 10ms/step - loss: 6.8145 - categorical_accuracy: 0.0443\n",
            "Epoch 4/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 6.4787 - categorical_accuracy: 0.0533\n",
            "Epoch 5/20\n",
            "3448/3448 [==============================] - 34s 10ms/step - loss: 6.1472 - categorical_accuracy: 0.0611\n",
            "Epoch 6/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 5.8516 - categorical_accuracy: 0.0640\n",
            "Epoch 7/20\n",
            "3448/3448 [==============================] - 34s 10ms/step - loss: 5.6038 - categorical_accuracy: 0.0654\n",
            "Epoch 8/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 5.4013 - categorical_accuracy: 0.0638\n",
            "Epoch 9/20\n",
            "3448/3448 [==============================] - 35s 10ms/step - loss: 5.2379 - categorical_accuracy: 0.0632\n",
            "Epoch 10/20\n",
            "3448/3448 [==============================] - 34s 10ms/step - loss: 5.1075 - categorical_accuracy: 0.0623\n",
            "Epoch 11/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 5.0029 - categorical_accuracy: 0.0618\n",
            "Epoch 12/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 4.9187 - categorical_accuracy: 0.0607\n",
            "Epoch 13/20\n",
            "3448/3448 [==============================] - 33s 9ms/step - loss: 4.8519 - categorical_accuracy: 0.0590\n",
            "Epoch 14/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 4.7983 - categorical_accuracy: 0.0582\n",
            "Epoch 15/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 4.7549 - categorical_accuracy: 0.0568\n",
            "Epoch 16/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 4.7187 - categorical_accuracy: 0.0574\n",
            "Epoch 17/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 4.6898 - categorical_accuracy: 0.0564\n",
            "Epoch 18/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 4.6653 - categorical_accuracy: 0.0552\n",
            "Epoch 19/20\n",
            "3448/3448 [==============================] - 34s 10ms/step - loss: 4.6443 - categorical_accuracy: 0.0558\n",
            "Epoch 20/20\n",
            "3448/3448 [==============================] - 33s 10ms/step - loss: 4.6265 - categorical_accuracy: 0.0553\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f718650f5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from keras.models import Input, Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "embed_size = 100\n",
        "\n",
        "inp = Input(shape=(X.shape[1],))\n",
        "x = Dense(units=embed_size, activation='linear')(inp)\n",
        "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics=['categorical_accuracy'])\n",
        "model.summary()\n",
        "model.fit(\n",
        "    x=X, \n",
        "    y=Y, \n",
        "    epochs=20,\n",
        "    verbose=1\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a dictionary to store the embeddings**"
      ],
      "metadata": {
        "id": "eQzHirhgPyZ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MzZNxEROCAn"
      },
      "outputs": [],
      "source": [
        "weights = model.get_weights()[0]\n",
        "\n",
        "embedding_dict = {}\n",
        "for word in words: \n",
        "    embedding_dict.update({\n",
        "        word: weights[unique_word_dict.get(word)]\n",
        "        })\n",
        "    \n",
        "embedding_dict   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **return similar words**"
      ],
      "metadata": {
        "id": "d3QaPcViUgDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4YswfHfLSH55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "b5f8b015-f6b5-4121-acd4-68ea3eb97492"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1fbc31e8bea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdistance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:15]+1] \n",
        "                   for search_term in ['گنج','خدا','دنیا']}\n",
        "\n",
        "similar_words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "word2vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQ9bXbXyffOaztcPM/S9QZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}