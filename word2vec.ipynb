{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPY2Y1BqGzdV79B6NOIrJeA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrn79/npl_wrod2wec/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OJ_ZJnknMjH",
        "outputId": "1b0d9602-63a4-43da-d0f8-6e9213445afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 41.6 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=ecee5422cfaccb701b72a03950da50233cfade0d5759d16e9f2313a3a05624f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154488 sha256=402ddfa9d65fdbb069e3bd1919326c2a750a3c2ff31ac764d2777ac42b62e247\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from keras.preprocessing import text"
      ],
      "metadata": {
        "id": "kBFYO3kdnN9X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7KKiOLfJmUxW"
      },
      "outputs": [],
      "source": [
        "with open('/content/shams.txt') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "with open('/content/stopwords.txt') as file:\n",
        "  stopLines = file.readlines()\n",
        "stopWord = [item.replace('\\n',\"\") for item in stopLines]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "words= []\n",
        "stemmer = Stemmer()\n",
        "for sent in lines :\n",
        "  words.append(word_tokenize(sent))\n",
        "\n",
        "text_word_list=[]\n",
        "corpes=[]\n",
        "new_words=[]\n",
        "\n",
        "for wordSent in words :\n",
        "  cleanSent=[]\n",
        "  for word in wordSent :\n",
        "    if word not in stopWord :\n",
        "      cleanSent.append(stemmer.stem(word))\n",
        "  if(len(cleanSent)>1) :\n",
        "    text_word_list+=cleanSent\n",
        "    new_words.append(cleanSent)\n",
        "\n",
        "value_of_repetition = dict(Counter(text_word_list))\n",
        "repetition=[]\n",
        "for word in value_of_repetition :\n",
        "  if value_of_repetition.get(word)==1:\n",
        "    repetition.append(word)\n",
        "clear_word_list=[x for x in text_word_list if x not in repetition]\n",
        "\n",
        "text_word_list=[]\n",
        "\n",
        "for wordSent in new_words :\n",
        "  cleanSent=[]\n",
        "  for word in wordSent :\n",
        "    if word not in repetition :\n",
        "      cleanSent.append(word)\n",
        "  if(len(cleanSent)>1) :\n",
        "    text_word_list+=cleanSent\n",
        "    corpes.append(' '.join(cleanSent))"
      ],
      "metadata": {
        "id": "ZfuBbbxUTMa_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpes)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}"
      ],
      "metadata": {
        "id": "p5y5E5G1UY3j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window = 2\n",
        "word_lists = []\n",
        "#text_word_list=[]\n",
        "\n",
        "for text in corpes:\n",
        "    text=word_tokenize(text)\n",
        "    # Creating a context dictionary\n",
        "    for i, word in enumerate(text):\n",
        "        for w in range(window):\n",
        "            # Getting the context that is ahead by *window* words\n",
        "            if i + 1 + w < len(text): \n",
        "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
        "            # Getting the context that is behind by *window* words    \n",
        "            if i - w - 1 >= 0:\n",
        "                word_lists.append([word] + [text[(i - w - 1)]])\n",
        "len(word_lists)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC77WNedoFY3",
        "outputId": "f1cea218-90fe-49fc-cf89-ede6e24fcbc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110306"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_set = list(set(text_word_list))\n",
        "words_set.sort()\n",
        "\n",
        "unique_word_dict = {}\n",
        "for i, word in enumerate(words_set):\n",
        "  unique_word_dict.update({\n",
        "    word: i\n",
        "  })\n",
        "\n",
        "#unique_word_dict\n",
        "len(unique_word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wtkv4ojtjdC",
        "outputId": "f608593b-5fac-4870-973f-5309d85f1090"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3447"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Defining the number of features (unique words)\n",
        "n_words = len(unique_word_dict)\n",
        "\n",
        "# Getting all the unique words \n",
        "words = list(unique_word_dict.keys())\n",
        "\n",
        "# Creating the X and Y matrices using one hot encoding\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i, word_list in tqdm(enumerate(word_lists)):\n",
        "    # Getting the indices\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\n",
        "\n",
        "    # Creating the placeholders   \n",
        "    X_row = np.zeros(n_words)\n",
        "    Y_row = np.zeros(n_words)\n",
        "\n",
        "    # One hot encoding the main word\n",
        "    X_row[main_word_index] = 1\n",
        "    # One hot encoding the Y matrix words \n",
        "    Y_row[context_word_index] = 1\n",
        "\n",
        "    # Appending to the main matrices\n",
        "    X.append(X_row)\n",
        "    Y.append(Y_row)\n",
        "\n",
        "# Converting the matrices into an array\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFm6FQJJyfXF",
        "outputId": "c156af34-71ca-4593-8d2a-285a10a0a406"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "110306it [00:05, 20719.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep learning: \n",
        "from keras.models import Input, Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Defining the size of the embedding\n",
        "embed_size = 100\n",
        "\n",
        "# Defining the neural network\n",
        "inp = Input(shape=(X.shape[1],))\n",
        "x = Dense(units=embed_size, activation='linear')(inp)\n",
        "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
        "\n",
        "# Optimizing the network weights\n",
        "model.fit(\n",
        "    x=X, \n",
        "    y=Y, \n",
        "    epochs=100,\n",
        "    verbose=2\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "29-u_XCcbR0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtaining the weights from the neural network. \n",
        "#These are the so called word embeddings\n",
        "\n",
        "#The input layer \n",
        "weights = model.get_weights()[0]\n",
        "\n",
        "#Creating a dictionary to store the embeddings in. The key is a unique word and \n",
        "#the value is the numeric vector\n",
        "embedding_dict = {}\n",
        "for word in words: \n",
        "    embedding_dict.update({\n",
        "        word: weights[unique_word_dict.get(word)]\n",
        "        })\n",
        "    \n",
        "embedding_dict   "
      ],
      "metadata": {
        "id": "2MzZNxEROCAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:15]+1] \n",
        "                   for search_term in ['گنج','خدا','دنیا']}\n",
        "\n",
        "similar_words"
      ],
      "metadata": {
        "id": "4YswfHfLSH55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}